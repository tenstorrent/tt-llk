# Getting started with writing and running tests

This document serves as a starting point for writing tests in the LLK repo. For more details on how the test infrastructure works in detail, read [this document](infra_architecture.md).
If your tests are failing for *no reason*, read our [debugging guide](debugging_guide.md)

## Table of contents
| | |
|:----|:----|
| 1 | Writing **functional** tests |
| 2 | Writing **performance** tests |
| 3 | Running the tests |
| 4 | Where do my compilation artifacts end up? |
| 5 | How do I see what code did my test cover? |
---

# 1. Writing Functional Tests

## Python part of the test

You should put your code in the `./tt-llk/tests/python_tests/` folder and name it using this template: `test_[my test name].py`. In the following example, `magnificent` will be an alias for `my test name`.

```python
from helpers.golden_generators import SomeGoldenGenerator
from helpers.llk_params import (format_dict, ... Some, Parameters ...)
from helpers.test_variant_parameters import (List,Of,Parameters)
from helpers.stimuli_config import StimuliConfig
from helpers.stimuli_generator import generate_stimuli
from helpers.test_config import TestConfig
from helpers.utils import passed_test


@parametrize(
	format = generate_the_format_list(...),
	parameter1 = generate_the_list_params_1(...),
	parameter2 = generate_the_list_params_2(...),
	parameter3 = generate_the_list_params_3(...)
	...
)

def test_magnificent(
	formats,
	... other magnificent parameters ...
	workers_tensix_coordinates
):
	src_A, tile_cnt_A, src_B, tile_cnt_B = generate_stimuli(
		stimuli_format_A=formats.input_format,
		input_dimensions_A=input_A_dimensions,
		stimuli_format_B=formats.input_format,
		input_dimensions_B=input_B_dimensions,
	)

	generate_golden = get_golden_generator(SomeGoldenGenerator)

	golden_tensor = generate_golden(
		src_A,
		src_B,
		formats.output_format,
		inA_dims= input A dimension provided as test parameter,
		inB_dims= input B dimension provided as test parameter,
	)

	configuration = TestConfig(
		"sources/magnificent_test.cpp",
		formats,
		templates=[ Template, Parameters, Here ],
		runtimes=[ Runtime, Parameters, Here ],
		variant_stimuli=StimuliConfig(
			src_A,
			formats.input_format,
			src_B,
			formats.input_format,
			formats.output_format,
			tile_count_A=tile_cnt_A,
			tile_count_B=tile_cnt_B,
			tile_count_res=tile_cnt_A,
		),
		...
		Other optional args that have default values
		...
	)

	res_from_L1 = configuration.run(workers_tensix_coordinates)

	assert len(res_from_L1) == len(golden_tensor), "Result tensor and golden tensor are not of the same length"

	res_tensor = torch.tensor(res_from_L1, dtype=format_dict[formats.output_format])

	assert passed_test(golden_tensor, res_tensor, formats.output_format), "Assert against golden failed"
```

### Arguments to the python test function that is to be parametrized

You shall add as many arguments as you wish, but `workers_tensix_coordinates` shall be present to enable use of multiple `tensix` cores to execute tests on the card itself. This argument is generated by the same named [pytest fixture](https://betterstack.com/community/guides/testing/pytest-fixtures-guide/#:~:text=Step%202%20%E2%80%94-,Getting%20started%20with%20Pytest%20fixtures,-To%20understand%20how).

### TestConfig object
This class and its object are used as a one-stop shop for building and executing all tests in the LLK infrastructure in a modular and performative way. The following snippet contains all available arguments exposed by the configuration:

```python
def TestConfig.__init__(
	self,
	test_name: str,
	formats: FormatConfig = None,
	templates: set[TemplateParameter] = None,
	runtimes: set[RuntimeParameter] = None,
	variant_stimuli: StimuliConfig = None,
	boot_mode: BootMode = BootMode.DEFAULT,
	profiler_build: ProfilerBuild = ProfilerBuild.No,
	L1_to_L1_iterations: int = 1,
	unpack_to_dest: bool = False,
	disable_format_inference: bool = False,
	dest_acc: DestAccumulation = DestAccumulation.No,
):
```

- `test_name` - a path to the `*.cpp` file of the test itself relative to `./tt-llk/tests` folder, this is the only mandatory argument
- `formats` - object that contains input and output formats used in the LLK test variants
- `templates` - a list of template parameters used in C++ LLK API calls, in the tests themselves
- `runtimes` - a list of parameters passed to all C++ LLK API functions in the tests themselves. These parameters will become fields of the struct that is passed as the `const volatile struct RuntimeParams *params` parameter of the kernel testing function itself.

Both `templates` and `runtimes` lists can be populated with object instances of classes defined in `helpers.test_variant_parameters`. These classes wrap types defined in `llk_params.py` and enable C++ code generation.

### StimuliConfig object

This class is used to:
- generate L1 addresses of the stimuli used by the kernel
- serialize (pack) the stimuli into a byte stream in accordance with provided formats
- write the byte stream to L1, and read the result from L1
- deserialize (unpack) the aforementioned byte stream and prepare it for assertion.

```python
def StimuliConfig.__init__(
	self,
	buffer_A,
	stimuli_A_format: DataFormat,
	buffer_B,
	stimuli_B_format: DataFormat,
	stimuli_res_format: DataFormat,
	tile_count_A: int = 1,
	tile_count_B: int = None,
	tile_count_res: int = 1,
	buffer_C=None,
	stimuli_C_format: DataFormat = None,
	tile_count_C: int = None,
	num_faces: int = 4,
	face_r_dim: int = 16,
	tile_dimensions: list[int] = [32, 32],
	sfpu=False,
):
```

- `buffer_[A|B|C|res]` - byte stream generated by `generate_stimuli` function
- `stimuli_[A|B|C|res]_formats` - data format of the respective stimuli
- `tile_count_[A|B|C|res]` - number of tiles present in the respective stimuli
- `num_faces`, `face_r_dim`, `tile_dimensions`, `sfpu` are used to further configure the way byte streams are packed into, and unpacked from L1.

### C++ part of the test

```cpp
... some magnificent header includes used by all kernels
#include "ckernel.h"
#include "llk_defs.h"


// Globals
... magnificent global variables used by all kernels

#ifdef LLK_TRISC_UNPACK
... magnificent header includes used by UNPACK part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	... magnificent kernel code ...
}

#endif


#ifdef LLK_TRISC_MATH
... magnificent header includes used by MATH part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	... magnificent kernel code ...
}

#endif

#ifdef LLK_TRISC_PACK
... magnificent header includes used by PACK part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	... magnificent kernel code ...
}

#endif
```

Named headers included in this example kernel code are mandatory to be included to ensure successful compilation. Importantly, `params.h` contains template argument definitions alongside `struct RuntimeParams` definition.

# 2. Writing Performance tests

## Python part of the tests

You should put your code in the `./tt-llk/tests/python_tests/` folder and name it using this template: `perf_[my test name].py`. In the following example, `magnificent` will be an alias for `my test name`.

```python
from helpers.llk_params import (PerfRunType, ... Other, Parameters ...)
from helpers.test_variant_parameters import (List,Of,Parameters)
from helpers.profiler import ProfilerConfig


@parametrize(
	format = generate_the_format_list(...),
	parameter1 = generate_the_list_params_1(...),
	parameter2 = generate_the_list_params_2(...),
	parameter3 = generate_the_list_params_3(...)
	...
)

def test_magnificent(
	perf_report
	formats,
	... other magnificent parameters ...
	workers_tensix_coordinates
):
 	configuration = ProfilerConfig(
		"sources/magnificent_test.cpp",
		formats,
		run_types=[ . . . My run types . . .],
		templates=[ Template, Parameters, Here ],
		runtimes=[ Runtime, Parameters, Here ],
		...
		Other optional args that have default values
		...
	)

	configuration.run(perf_report, location=workers_tensix_coordinates)
```

### Arguments to the python test function that is to be parametrized

You shall add as many arguments as you wish, but `workers_tensix_coordinates` and `perf_report` shall always be present. The first enables use of multiple Tensix cores to execute tests on the card itself. The second enables generation of performance reports. Both arguments are generated by the same named [pytest fixtures](https://betterstack.com/community/guides/testing/pytest-fixtures-guide/#:~:text=Step%202%20%E2%80%94-,Getting%20started%20with%20Pytest%20fixtures,-To%20understand%20how).

### ProfilerConfig object

```python
def ProfilerConfig.__init__(
	self,
	test_name: str,
	formats: FormatConfig = None,
	run_types: list[PerfRunType] = None,
	templates: set[TemplateParameter] = None,
	runtimes: set[RuntimeParameter] = None,
	variant_stimuli: StimuliConfig = None,
	unpack_to_dest=False,
	disable_format_inference=False,
	dest_acc=DestAccumulation.No,
):
```

Every argument present in [[#TestConfig object]] has the exact same functionality as in this object. The only difference is the `run_types` argument which specifies with what performance markers the tests are compiled with, thus these are treated the same way as template parameters. Available run types are defined in `python_tests/helpers/llk_params.py:PerfRunType(Enum)`.

## C++ part of the performance test

```cpp
... some magnificent header includes used by all kernels
#include "ckernel.h"
#include "llk_defs.h"


// Globals
... magnificent global variables used by all kernels

#ifdef LLK_TRISC_UNPACK
... magnificent header includes used by UNPACK part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	{
		ZONE_SCOPED("INIT")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
	{
		ZONE_SCOPED("TILE_LOOP")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
}

#endif


#ifdef LLK_TRISC_MATH
... magnificent header includes used by MATH part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	{
		ZONE_SCOPED("INIT")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
	{
		ZONE_SCOPED("TILE_LOOP")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
}

#endif

#ifdef LLK_TRISC_PACK
... magnificent header includes used by PACK part of the kernel ...

#include "params.h"
void run_kernel(const volatile struct RuntimeParams *params){
	{
		ZONE_SCOPED("INIT")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
	{
		ZONE_SCOPED("TILE_LOOP")
		... magnificent kernel code to be profiled ...
		PROFILER_SYNC();
	}
}

#endif
```

# 3. Running the tests

## Setting up the environment
If you're using LLK IRD Docker image, `cd` yourself to `./tt-llk/tests` and run:

`./setup_testing_env.sh`

This will install all the necessary headers for your platform alongside latest `SFPI` version.

## Invoking pytest
`cd` yourself to one of the paths:
- `./tt-llk/tests`
- `./tt-llk/tests/python_tests/`

afterwards run:

`pytest ./path/to/my_test_name.py`

This command will serially compile and execute your test's variants. This is okay if you have few variants (<20), because this approach takes about ~1s per variant when accounted for python overheads.

If you have many variants that need to compile, you should run

`pytest --compile-producer -n 10 -x ./my_test_name.py`

which will first compile all your variants using 10 CPU cores of your host machine. Then you run all your compiled tests with

`pytest --compile-consumer -x ./my_test_name.py`

You can run your pre-compiled tests as many times as you like. This is useful when you're experimenting with different **runtime arguments** or **stimuli generation**
functions. Keep in mind that if you change any other parameters (eg. format, templates or other arguments to constructors) you **must** recompile your test variant by running the first `pytest` command. If you don't do that, you'll probably get `TTException` that states that the elf file can't be found. If you don't get it, you'll get a stimuli assertion error.

### Collecting coverage data

For collecting code coverage data, you just pass `--coverage` flag to both `pytest` commands. If you want to collect coverage, you **must** compile your variants with coverage flag enabled to be able to collect that information.

# 4. Where do my compilation artifacts end up?

Default build directory is at `/tmp/tt-llk-build/`. Your test's artifacts will be at the same path as the one provided in the `test_name` argument in the `TestConfig`/`ProfilerConfig` object. All variants will be placed in folders corresponding to the hash of their compile time arguments. Header file `build.h` generated using passed configuration parameters is placed in its variant's folder.
If coverage is enabled, all coverage data is merged at the end of execution of tests and placed in the `/tmp/tt-llk-build/merged_coverage.info` file.

# 5. How do I see what code did my test(s) cover?

After finishing compilation and execution of your tests with coverage enabled, run `./generate_coverage_report.sh` placed in the `./tt-llk/tests` folder. The generated report is placed in the same folder as your `./tt-llk` repo. For convenient preview of the coverage data, I'd recommend installing the [Live Server VSCode extension](https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer). When it opens your browser tab, you'll see the `./coverage_report` folder.
