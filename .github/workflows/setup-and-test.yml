name: "âš™ï¸ Setup and Test"

on:
  workflow_call:
    inputs:
      docker_image:
        description: "The Docker image to use for the container"
        required: true
        type: string
      runs_on:
        description: "The runner to use for the job"
        required: true
        type: string
      test_splits:
        description: "Number of test groups to split tests into (e.g., 4)"
        required: true
        type: number
      pytest_markers:
        description: "Pytest mark expression to select/deselect tests (e.g., 'not perf', 'nightly or not perf')"
        required: false
        default: "not perf"
        type: string
      timeout_minutes:
        description: "Timeout in minutes for the test job"
        required: false
        default: 80
        type: number
      random_order:
        description: "Enable randomized test order"
        required: false
        default: false
        type: boolean
      coverage:
        description: "Enable code coverage testing for entire test suite"
        required: false
        default: false
        type: boolean

permissions:
  checks: write

jobs:
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: |
          matrix=$(echo "[$(seq -s, 1 ${{ inputs.test_splits }})]")
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

  setup-and-test:
    needs: generate-matrix
    runs-on: ${{ inputs.runs_on }}
    timeout-minutes: ${{ inputs.timeout_minutes }}
    strategy:
      fail-fast: false
      matrix:
        test_group: ${{ fromJSON(needs.generate-matrix.outputs.matrix) }}
    container:
      image: harbor.ci.tenstorrent.net/${{ inputs.docker_image }}
      options: "--rm --device /dev/tenstorrent --ulimit nofile=4096:4096"
    name: "ðŸ¦„ Run tests (group ${{ matrix.test_group }}/${{ inputs.test_splits }})"
    steps:
      # Step 1: Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Install SFPI
      - name: Install SFPI
        shell: bash
        run: |
          cd tests
          ./setup_testing_env.sh
          cd ..

      # Step 3: Run the tests
      - name: Run tests
        shell: bash
        env:
          TEST_SPLITS: ${{ inputs.test_splits }}
        run: |
          CHIP_ARCH=unknown
          if [[ "${{ inputs.runs_on }}" =~ n150 ]]; then
            CHIP_ARCH=wormhole
          elif [[ "${{ inputs.runs_on }}" =~ p150 ]]; then
            CHIP_ARCH=blackhole
          fi
          export CHIP_ARCH
          echo "CHIP_ARCH=$CHIP_ARCH" >> $GITHUB_ENV
          SPLITS=$TEST_SPLITS
          cd tests/python_tests/

          COVERAGE_FLAG=""
          if [[ "${{ inputs.coverage }}" == "true" ]]; then
            COVERAGE_FLAG="--coverage"
          fi

          pytest -m "${{ inputs.pytest_markers }}" \
                ${{ inputs.random_order && '--random-order-bucket=global' || '' }} \
                --splits $SPLITS --group ${{ matrix.test_group }} \
                --override-ini="addopts=-v" --timeout=60 \
                $COVERAGE_FLAG \
                --junitxml=pytest-report-${CHIP_ARCH}-${{ matrix.test_group }}.xml . || true

          if [[ "${{ inputs.coverage }}" == "true" ]]; then
            cd ../
            make merge_coverage_data
          fi

      # Step 4: Zip performance data if perf tests were run
      - name: Zip performance data
        if: always() && contains(inputs.pytest_markers, 'perf') && hashFiles('perf_data/**') != ''
        shell: bash
        run: |
          zip -r perf_data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.zip perf_data/
          echo "Performance data zipped successfully"

      # Step 4.1: Upload generated info file per runner
      - name: Upload code coverage for this runner
        uses: actions/upload-artifact@v4
        if: always() && inputs.coverage
        with:
          name: code-coverage-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.info
          path: tests/build/${{ env.CHIP_ARCH }}/merged_coverage.info

      # Step 5: Upload performance data
      - name: Upload performance data
        uses: actions/upload-artifact@v4
        if: always() && contains(inputs.pytest_markers, 'perf') && hashFiles('perf_data-*.zip') != ''
        with:
          name: perf-data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}
          path: perf_data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.zip

      # Step 6: Upload the JUnit report
      - name: Upload JUnit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: >-
            ${{ (contains(inputs.pytest_markers, 'perf') && !contains(inputs.pytest_markers, 'not perf'))
                && 'perf-' || '' }}junit-report-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}
          path: tests/python_tests/pytest-report-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.xml

      # Step 7: Publish the test results
      - name: Publish Test Results
        uses: mikepenz/action-junit-report@v5
        if: always()
        with:
          report_paths: tests/python_tests/pytest-report-*-${{ matrix.test_group }}.xml
          include_passed: true
          annotate_only: true

  merge-coverage:
    name: "Merge coverage files"
    runs-on: ubuntu-latest
    container:
      image: ${{ inputs.docker_image }}
    needs: setup-and-test
    if: inputs.coverage == true
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: code-coverage-*
          path: ./merged_coverage_files
      - name: Unzip artefacts, merge coverage data, and generate HTML report
        run: |
          mkdir -p ./extracted_coverage_files
          for dir in ./merged_coverage_files/*/; do
            if [ -d "$dir" ]; then
              folder_name=$(basename "$dir")
              if [ -f "$dir/merged_coverage.info" ]; then
                cp "$dir/merged_coverage.info" "./extracted_coverage_files/${folder_name}"
              fi
            fi
          done

          echo "-- Coverage files pulled from runners --"
          find ./extracted_coverage_files/ -name "*.info"

          first=$(ls ./extracted_coverage_files | head -n 1)
          cp ./extracted_coverage_files/"$first" merged.info

          for f in $(find ./extracted_coverage_files/ -name "*.info"); do
            if [ "$f" != ./extracted_coverage_files/"$first" ]; then
              lcov -q -a merged.info -a "$f" -o ./merged.info
            fi
          done

          lcov -q --remove ./merged.info '*tests/*' \
            -o ./merged_filtered.info

          mkdir -p coverage_html
          genhtml --branch-coverage ./merged_filtered.info --output-directory coverage_html
      - name: Upload code coverage for this runner
        uses: actions/upload-artifact@v4
        with:
          name: coverage_report_website
          path: ./coverage_html
