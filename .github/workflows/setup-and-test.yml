name: "⚙️ Setup and Test"

on:
  workflow_call:
    inputs:
      docker_image:
        description: "The Docker image to use for the container"
        required: true
        type: string
      runs_on:
        description: "The runner to use for the job"
        required: true
        type: string
      test_splits:
        description: "Number of test groups to split tests into (e.g., 4)"
        required: true
        type: number
      pytest_markers:
        description: "Pytest mark expression to select/deselect tests (e.g., 'not perf', 'nightly or not perf')"
        required: false
        default: "not perf"
        type: string
      timeout_minutes:
        description: "Timeout in minutes for the test job"
        required: false
        default: 80
        type: number

permissions:
  checks: write

jobs:
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: |
          matrix=$(echo "[$(seq -s, 1 ${{ inputs.test_splits }})]")
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

  setup-and-test:
    needs: generate-matrix
    runs-on: ${{ inputs.runs_on }}
    timeout-minutes: ${{ inputs.timeout_minutes }}
    strategy:
      fail-fast: false
      matrix:
        test_group: ${{ fromJSON(needs.generate-matrix.outputs.matrix) }}
    container:
      image: harbor.ci.tenstorrent.net/${{ inputs.docker_image }}
      options: "--rm --device /dev/tenstorrent"
    name: "🦄 Run tests (group ${{ matrix.test_group }}/${{ inputs.test_splits }})"
    steps:
      # Step 1: Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Install SFPI
      - name: Install SFPI
        shell: bash
        run: |
          cd tests
          ./setup_testing_env.sh
          cd ..

      # Step 3: Run the tests
      - name: Run tests
        shell: bash
        env:
          TEST_SPLITS: ${{ inputs.test_splits }}
        run: |
          CHIP_ARCH=unknown
          if [[ "${{ inputs.runs_on }}" =~ n150 ]]; then
            CHIP_ARCH=wormhole
          elif [[ "${{ inputs.runs_on }}" =~ p150 ]]; then
            CHIP_ARCH=blackhole
          fi
          echo "CHIP_ARCH=$CHIP_ARCH" >> $GITHUB_ENV
          SPLITS=$TEST_SPLITS
          cd tests/python_tests/
          pytest -m "${{ inputs.pytest_markers }}" \
                --splits $SPLITS --group ${{ matrix.test_group }} \
                --override-ini="addopts=-v" --timeout=60 \
                --junitxml=pytest-report-${CHIP_ARCH}-${{ matrix.test_group }}.xml .

      # Step 4: Zip performance data if perf tests were run
      - name: Zip performance data
        if: always() && contains(inputs.pytest_markers, 'perf') && hashFiles('perf_data/**') != ''
        shell: bash
        run: |
          zip -r perf_data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.zip perf_data/
          echo "Performance data zipped successfully"

      # Step 5: Upload performance data
      - name: Upload performance data
        uses: actions/upload-artifact@v4
        if: always() && contains(inputs.pytest_markers, 'perf') && hashFiles('perf_data-*.zip') != ''
        with:
          name: perf-data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}
          path: perf_data-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.zip

      # Step 6: Upload the JUnit report
      - name: Upload JUnit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: junit-report-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}
          path: tests/python_tests/pytest-report-${{ env.CHIP_ARCH }}-${{ matrix.test_group }}.xml

      # Step 7: Publish the test results
      - name: Publish Test Results
        uses: mikepenz/action-junit-report@v5
        if: always()
        with:
          report_paths: tests/python_tests/pytest-report-*-${{ matrix.test_group }}.xml
          include_passed: true
          annotate_only: true
