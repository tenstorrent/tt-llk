// LLK AI Issue Consultant - TT-Chat API Integration
//
// This script calls the Tenstorrent TT-Chat API using OpenAI-compatible client
// to perform LLK analysis on GitHub issues.

import * as core from '@actions/core';
import {writeFileSync} from 'fs';
import {OpenAI} from 'openai';

// Environment variables
const apiKey       = process.env.TT_CHAT_API_KEY;
const baseUrl      = process.env.TT_CHAT_BASE_URL;
const model        = process.env.TT_CHAT_MODEL;
const prompt       = process.env.AI_PROMPT;
const promptSource = process.env.PROMPT_SOURCE || 'unknown';
const issueTitle   = process.env.ISSUE_TITLE;
const issueAuthor  = process.env.ISSUE_AUTHOR;
const issueLabels  = process.env.ISSUE_LABELS;

async function main()
{
    console.log('🚀 Starting TT-Chat API call for LLK analysis...');

    try
    {
        // Validate required environment variables
        if (!apiKey)
        {
            throw new Error('TT_CHAT_API_KEY environment variable is required');
        }
        if (!baseUrl)
        {
            throw new Error('TT_CHAT_BASE_URL environment variable is required');
        }
        if (!model)
        {
            throw new Error('TT_CHAT_MODEL environment variable is required');
        }
        if (!prompt)
        {
            throw new Error('AI_PROMPT environment variable is required');
        }

        console.log(`🔧 Configuration:`);
        console.log(`   Base URL: ${baseUrl}`);
        console.log(`   Model: ${model}`);
        console.log(`   Prompt Source: ${promptSource}`);
        console.log(`   Issue: ${issueTitle}`);
        console.log(`   Author: ${issueAuthor}`);
        console.log(`   Labels: ${issueLabels}`);

        // Initialize OpenAI client with TT-Chat configuration
        const client = new OpenAI({apiKey: apiKey, baseURL: baseUrl});

        console.log('🔍 Sending LLK analysis request to TT-Chat...');
        console.log(`📝 Prompt (${promptSource}): ${prompt.substring(0, 200)}...`);

        // Create chat completion request
        const response = await client.chat.completions.create({
            messages: [
                {
                    role: 'system',
                    content:
                        'You are an expert in Tenstorrent\'s Low Level Kernel (LLK) APIs and Tensix processor architecture. Provide detailed, technical analysis focused on LLK APIs, Tensix instructions, and configuration registers. Be specific and reference actual code patterns when possible.'
                },
                {role: 'user', content: prompt}
            ],
            model: model,
            stream: true,
            temperature: 0.1, // Lower temperature for more consistent technical analysis
            max_tokens: 2000  // Sufficient for detailed analysis
        });

        console.log('📡 Receiving streaming response from TT-Chat...');

        // Collect streaming response
        const fullContent = [];
        for await (const chunk of response)
        {
            const chunkContent = chunk.choices[0]?.delta?.content;
            if (chunkContent)
            {
                fullContent.push(chunkContent);
                // Log progress every 50 chunks to avoid too much output
                if (fullContent.length % 50 === 0)
                {
                    console.log(`📥 Received ${fullContent.length} response chunks...`);
                }
            }
        }

        const aiAnalysis = fullContent.join('');

        if (!aiAnalysis || aiAnalysis.trim().length === 0)
        {
            throw new Error('Received empty response from TT-Chat API');
        }

        console.log(`✅ TT-Chat analysis completed (${aiAnalysis.length} characters)`);
        console.log(`📊 First 200 characters: ${aiAnalysis.substring(0, 200)}...`);

        // Get prompt source description for the response
        const promptSourceDesc = promptSource === 'generated' ? '🧠 AI-Generated Specialized Prompt' :
                                 promptSource === 'default'   ? '🔄 Default Prompt (AI generation failed)' :
                                                                '⚙️ Hardcoded Default Prompt';

        // Format the AI response for posting
        const formattedResponse = `🤖 **LLK AI Consultant Analysis**

**Issue:** ${issueTitle}
**Reporter:** @${issueAuthor}
**Labels:** ${issueLabels}
**Prompt:** ${promptSourceDesc}

---

## TT-Chat Analysis

${aiAnalysis}

---

*Analysis generated by TT-Chat (${model}) on ${new Date().toISOString()}*
*Powered by Tenstorrent's LLK-focused AI consultant*`;

        // Write the response to file for the post-comment script
        writeFileSync('ai_response.md', formattedResponse, 'utf8');

        console.log('📝 AI response written to ai_response.md');
        console.log('✅ TT-Chat API call completed successfully');

        return {success: true, responseLength: aiAnalysis.length, model: model};
    }
    catch (error)
    {
        console.error('❌ TT-Chat API call failed:', error.message);

        // Create fallback error response
        const promptSourceDesc = promptSource === 'generated' ? '🧠 AI-Generated Specialized Prompt' :
                                 promptSource === 'default'   ? '🔄 Default Prompt (AI generation failed)' :
                                                                '⚙️ Hardcoded Default Prompt';

        const errorResponse = `🤖 **LLK AI Consultant Analysis**

**Issue:** ${issueTitle || 'Unknown'}
**Reporter:** @${issueAuthor || 'Unknown'}
**Labels:** ${issueLabels || 'Unknown'}
**Prompt:** ${promptSourceDesc}

---

## ⚠️ Analysis Error

Unfortunately, the TT-Chat AI analysis could not be completed due to a technical issue:

\`\`\`
${error.message}
\`\`\`

### Fallback Analysis Structure

The issue will be manually analyzed according to the following structure:

### 1. Relevant LLK APIs
*Manual analysis required*

### 2. Tensix Instructions Called
*Manual analysis required*

### 3. Tensix Configuration Registers Programmed
*Manual analysis required*

---

*Error occurred on ${new Date().toISOString()}*
*Please check TT-Chat API configuration and try again*`;

        // Write error response to file
        writeFileSync('ai_response.md', errorResponse, 'utf8');

        console.log('📝 Error response written to ai_response.md');

        // Don't throw - let the workflow continue with error response
        return {success: false, error: error.message};
    }
}

// Run the main function when script is executed directly
if (import.meta.url === `file://${process.argv[1]}`)
{
    main().catch(error => {
        core.setFailed(`Script failed: ${error.message}`);
        process.exit(1);
    });
}

export default main;
